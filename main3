import tensorflow as tf
from tensorflow.keras.datasets import mnist
import numpy as np

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

print(f"Training data shape: {x_train.shape}")
# Output: (60000, 28, 28) - 60k images, 28x28 pixels
# Normalize image data
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Reshape for CNN input (Add a channel dimension of 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

# One-hot encode the target labels (10 classes: 0 to 9)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

def create_cnn_model(input_shape=(28, 28, 1), num_classes=10):
    model = Sequential([
        # First Convolutional Block
        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),
        MaxPooling2D(pool_size=(2, 2)),

        # Second Convolutional Block
        Conv2D(64, kernel_size=(3, 3), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),

        # Flatten the 3D output to a 1D vector
        Flatten(),

        # Dense Classification Layers
        Dense(128, activation='relu'),
        Dense(num_classes, activation='softmax') # Output layer
    ])
    return model

# Create and compile the model
model = create_cnn_model()

# Compile the model with an optimizer, loss function, and metrics
model.compile(optimizer='adam',
              loss='categorical_crossentropy', # For multi-class classification
              metrics=['accuracy'])

model.summary()
# Train the model
# epochs: The number of times the model will see the entire training dataset
# batch_size: The number of samples per gradient update
history = model.fit(x_train, y_train,
                    batch_size=128,
                    epochs=10,
                    validation_data=(x_test, y_test))

# Evaluate the model on the test data
loss, accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Example Prediction
# Get the first 10 test images
predictions = model.predict(x_test[:10])

# Convert one-hot predictions to class labels (0-9)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test[:10], axis=1)

print("\n--- Example Predictions ---")
print(f"True Labels:      {true_classes}")
print(f"Predicted Labels: {predicted_classes}")